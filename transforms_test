from transformers import DistilBertTokenizerFast
import torch

classification = ['neg','pos']
save_path = './savedModels/best_so_far.pth'
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
model = torch.load(save_path).cpu()
s0 = '''I was expecting a lot better from the Battlestar Galactica franchise. Very boring prequel to the main series. After the first 30 minutes, I was waiting for it to end. The characters do a lot of talking about religion, computers, programming, retribution, etc... There are gangsters, mafia types, who carry out hits. However, Caprica doesn't have the action of the original series to offset the slower parts.Let me give you some helpful advice when viewing movies: As a general rule, if there is a lot of excessive exploitive titillation, then you know the movie will be a dud. Caprica has lots of this. The director/writer usually attempts to compensate for his poor abilities by throwing in a few naked bodies. It never works and all it does is demean the (very) young actresses involved and I feel sorry for them. Directors/writers who do this should be banned from the business.If you want to be bored for an hour and a half, by all means, rent Caprica. There's (free) porn on the 'Net if you really want to see naked bodies. Otherwise, move along, nothing to see here.'''
sentence_encoding = tokenizer(s0,truncation=True, padding=True,return_tensors="pt")
model_output = model(**sentence_encoding)[0]
result = torch.softmax(model_output,dim=1)[0]
idx = torch.argmax(result).item()
pred = classification[idx]
print(sentence)
print(pred)
